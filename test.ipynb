{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set api & llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('.dev.yaml','r') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = CONFIG['openai_api_key'][0]\n",
    "os.environ[\"SERPAPI_API_KEY\"] = CONFIG['serpapi_api_key']\n",
    "os.environ[\"PINECONE_API_KEY\"] = CONFIG['pinecone_api_key']  # find at app.pinecone.io\n",
    "os.environ[\"PINECONE_ENVIROMENT\"] = CONFIG['pinecone_environment']  # next to api key in console\n",
    "#print(CONFIG)\n",
    "#print(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "promptObj = PromptTemplate(input_variables=[\"name\",\"what\"],template=\"{name}{what}有多少？\")\n",
    "final_prompt=promptObj.format(name=\"香港\",what=\"面积\")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "import random\n",
    "def get_llm(**kwargs):\n",
    "    name = kwargs.get('name', 'openai')\n",
    "    is_stream = kwargs.get('is_stream', True)\n",
    "    openai_api_key = kwargs.get('openai_api_key', None)\n",
    "    model_name = kwargs.get('model_name', 'gpt-3.5-turbo')\n",
    "    temperature = kwargs.get('temperature', 0.7)\n",
    "    if openai_api_key==None:\n",
    "        #openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "        openai_api_key = random.choice(CONFIG[\"openai_api_key\"])\n",
    "    llm = None\n",
    "    if name==\"openai\":\n",
    "        if is_stream:\n",
    "            llm = OpenAI(streaming=True,\n",
    "                         openai_api_key = openai_api_key,\n",
    "                         callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                         model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "        else:\n",
    "            llm = OpenAI(openai_api_key = openai_api_key,model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "    else:\n",
    "        name=\"chat\"\n",
    "        if is_stream:\n",
    "            llm = ChatOpenAI(streaming=True, \n",
    "                             openai_api_key = openai_api_key,\n",
    "                             callbacks=[StreamingStdOutCallbackHandler()], \n",
    "                             temperature=0)\n",
    "        else:\n",
    "            llm = ChatOpenAI(openai_api_key = openai_api_key,temperature=0)\n",
    "    return llm\n",
    "stream_chat = get_llm(name=\"chat\",is_stream=True)\n",
    "chat = get_llm(name=\"chat\")\n",
    "stream_openai = get_llm(name=\"openai\",is_stream=True)\n",
    "openai=get_llm(name=\"openai\",is_stream=False)\n",
    "openai(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai(prompt=prompt.format(name=\"美国\"))\n",
    "promptObj = PromptTemplate(input_variables=[\"name\",\"what\"],template=\"{name}{what}有多少？\")\n",
    "messages = [\n",
    "    SystemMessage(content=\"用中文和英文两种语言回答\"),\n",
    "    HumanMessage(content=promptObj.format(name=\"上海\",what=\"天气\"))\n",
    "]\n",
    "result1 = stream_openai(promptObj.format(name=\"上海\",what=\"天气\"))\n",
    "print(\"\\n stream_openai \\n\",result1)\n",
    "result2 = openai.generate([promptObj.format(name=\"上海\",what=\"天气\")])\n",
    "print(\"\\n openai \\n\",result2)\n",
    "result3 = stream_chat(messages)\n",
    "print(\"\\n stream_chat \\n\",result3)\n",
    "result4 = chat.generate([messages])\n",
    "print(\"\\n chat \\n\",result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "promptObj = PromptTemplate(input_variables=[\"name\",\"what\"],template=\"{name}{what}有多少？\")\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=stream_openai , prompt=promptObj)\n",
    "chain.run({\"name\":\"香港\",\"what\":\"人口\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain\n",
    "chain.save('test_chain.json')\n",
    "chain1 = load_chain('test_chain.json')\n",
    "chain1.run({\"name\":\"全世界\",\"what\":\"专利数\"})\n",
    "#出错原因是load_chain目前不支持chatgpt，仅支持openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SequentialChain\n",
    "只有一个input，一个output可以使用SimpleSequentialChain。更通用的情况使用SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"根据给定title和name，写一首关于name的诗歌\n",
    "\n",
    "Title: {title}\n",
    "Name: {name}\n",
    "Playwright: 以下是诗歌:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"title\", 'name'], template=template)\n",
    "chainA = LLMChain(llm=stream_openai, prompt=prompt_template, output_key=\"poem\")\n",
    "\n",
    "# This is an LLMChain to write a review of a play given a synopsis.\n",
    "template = \"\"\"根据给定的poem，自己评价自己写的好不好\n",
    "Poem:\n",
    "{poem}\n",
    "对这个诗歌的评价:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"poem\"], template=template)\n",
    "chainB = LLMChain(llm=stream_openai, prompt=prompt_template, output_key=\"review\")\n",
    "\n",
    "chainC = LLMChain.from_string(llm=stream_openai, template=\"根据{poem},把{name}替换为{other}\")\n",
    "\n",
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SequentialChain\n",
    "all_chain = SequentialChain(\n",
    "    chains=[chainA, chainB,chainC],\n",
    "    input_variables=[\"title\", \"name\",\"other\"],\n",
    "    # Here we return multiple variables\n",
    "    output_variables=[\"poem\", \"review\"],\n",
    "    verbose=True)\n",
    "\n",
    "result = all_chain({\"title\":\"我家的小狗\", \"name\": \"狗\",\"other\":\"猫\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RouteChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./nianbao/ylz2022.pdf\")\n",
    "documents = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "import pickle\n",
    "index_name='ylz_nianbao'\n",
    "embeddings = OpenAIEmbeddings()\n",
    "store = FAISS.from_documents(documents, embeddings)\n",
    "faiss.write_index(store.index, index_name)\n",
    "store.index = None\n",
    "with open(\"ylz_nianbao.pkl\",\"wb\") as f:\n",
    "    pickle.dump(store, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "openai = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "index = faiss.read_index(\"ylz_nianbao\")\n",
    "with open(\"ylz_nianbao.pkl\",\"rb\") as f:\n",
    "  docsearch = pickle.load(f)\n",
    "  docsearch.index = index\n",
    "query=\"公司全年亏损多少？什么原因？\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "print(len(docs))\n",
    "chain = load_qa_chain(openai, chain_type=\"refine\")\n",
    "r = chain.run(input_documents=docs,question=f\"以下用中文回答\\n\\n{query}\")\n",
    "print(r.encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "urls = [\n",
    "    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n",
    "    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\"\n",
    "]\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=openai)\n",
    "#agent = initialize_agent(tools, openai, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"易联众300096昨天收盘多少钱?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "conversation = ConversationChain(llm=stream_openai, verbose=True)\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)\n",
    "output = conversation.predict(input=\"what do I say just now?\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.chat_memory.add_user_message(\"你好\")\n",
    "memory.chat_memory.add_ai_message(\"你好，很高兴见到你\")\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "dicts = messages_to_dict(memory.load_memory_variables({})[\"history\"])\n",
    "print(dicts)\n",
    "\n",
    "new_memory = messages_from_dict(dicts)\n",
    "conversation = ConversationChain(llm=stream_openai,memory=new_memory)\n",
    "conversation.predict(input=\"介绍你自己\")\n",
    "# from langchain.schema import AIMessage,HumanMessage,SystemMessage,messages_to_dict,messages_from_dict\n",
    "# messages=[SystemMessage(content=\"随机增加几个emoji字符\"),HumanMessage(content=\"如何理解langchain\")]\n",
    "# chat(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "memory = ConversationSummaryBufferMemory(llm=openai, max_token_limit=40)\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=stream_openai, \n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"猫和狗都是动物吗\")\n",
    "conversation_with_summary.predict(input=\"鸭子也是吗\")\n",
    "conversation_with_summary.predict(input=\"我刚才说了几种动物\")\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import langchain.text_splitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "def summarize_docs(docs,doc_url):\n",
    "        print(f'you have {len(docs)} documents in your {doc_url} data')\n",
    "        print(f'there are {len(docs[0].page_content)} characters in your document')\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        print(f'you have {len(split_docs)} split documents')\n",
    "        llm = ChatOpenAI()\n",
    "        chain = load_summarize_chain(llm,chain_type=\"map_reduce\",verbose=True)\n",
    "        response=\"\"\n",
    "        with get_openai_callback() as cb:\n",
    "            response = chain.run(input_documents=split_docs)\n",
    "            print(cb)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "url = \"https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker\"\n",
    "summarize_docs(UnstructuredURLLoader(urls=[url]).load(),url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI,SQLDatabase,SQLDatabaseChain\n",
    "db = SQLDatabase.from_uri(\"sqlite:///address.db\")\n",
    "print(db.table_info)\n",
    "db_chain = SQLDatabaseChain(llm=openai,database=db,verbose=True)\n",
    "db_chain.run(\"平均年龄是多少\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA with pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import TextLoader,DirectoryLoader\n",
    "\n",
    "#loader = TextLoader('langchainDoc')\n",
    "loader = DirectoryLoader('./', glob=\"**/*.html\", show_progress=True)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "import pinecone \n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=config['pinecone_api_key'],  # find at app.pinecone.io\n",
    "    environment=config['pinecone_environment']  # next to api key in console\n",
    ")\n",
    "index_name = \"langchain-demo\"\n",
    "docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "# if you already have an index, you can load it like this\n",
    "# docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from tqdm.autonotebook import tqdm\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "import pinecone\n",
    "pinecone.init(\n",
    "    api_key=config['pinecone_api_key'],  # find at app.pinecone.io\n",
    "    environment=config['pinecone_environment']  # next to api key in console\n",
    ")\n",
    "index_name = \"langchain-demo\"\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "promptTemplate = PromptTemplate(input_variables=[\"query\"], template=\"请用中文回答以下问题:\\n{query}\")\n",
    "query = \"load_qa_chain使用的是什么技术\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "#print(docs)\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(openai, chain_type=\"refine\")\n",
    "chain.run(input_documents=docs, question=promptTemplate.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader,DirectoryLoader\n",
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "def load_train(name, dir=\"./\", type=\"html\"):\n",
    "  file = f\"{name}.pkl\"\n",
    "  if not os.path.exists(file):\n",
    "      #loader = TextLoader('langchainDoc')\n",
    "      if type=='pdf':\n",
    "            loader = PyPDFDirectoryLoader(dir)\n",
    "      else:\n",
    "            loader = DirectoryLoader(dir, glob=f\"**/*.{type}\", show_progress=True)\n",
    "      documents = loader.load()\n",
    "      print(f\"find {len(documents)} documents\")\n",
    "      text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "      docs = text_splitter.split_documents(documents) \n",
    "      embeddings = OpenAIEmbeddings()\n",
    "      store = FAISS.from_documents(docs, embeddings)\n",
    "      faiss.write_index(store.index, name)\n",
    "      store.index = None\n",
    "      with open(file,\"wb\") as f:\n",
    "            pickle.dump(store, f)\n",
    "  \n",
    "  index = faiss.read_index(name)\n",
    "  with open(file,\"rb\") as f:\n",
    "      docsearch = pickle.load(f)\n",
    "      docsearch.index = index\n",
    "  return docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "name=\"langchainDoc\"\n",
    "dir=\"./python.langchain.com/\"\n",
    "type=\"html\"\n",
    "docsearch = load_train(name,dir,type)\n",
    "print(f\"we have docsearch named:{name}\")\n",
    "query=\"who to use few shot？\"\n",
    "promptTemplate = PromptTemplate(input_variables=[\"query\"],template=\"以下请用中文回答，尽量采用表格或列表回答\\n{query}\")\n",
    "find_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain = load_qa_chain(stream_openai, chain_type=\"refine\")\n",
    "r = chain.run(input_documents=find_docs,question=promptTemplate.format(query=query))\n",
    "print(r.encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = load_train(\"ylz-nianbao\",\"pdf\")\n",
    "#query=\"langchain有哪些模块，如何自定义tools，给出代码\"\n",
    "query=\"公司亏损多少，是什么原因？\"\n",
    "promptTemplate = PromptTemplate(input_variables=[\"query\"],template=\"以下请用中文回答，尽量采用有条理的回答，尽量提供具体代码\\n{query}\")\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "r = chain.run(input_documents=docs,question=promptTemplate.format(query=query))\n",
    "print(r.encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "name=\"zhuanli\"\n",
    "dir=\"./深度学习与卷积神经网络相关/\"\n",
    "type=\"pdf\"\n",
    "docsearch = load_train(name,dir,type)\n",
    "print(docsearch)\n",
    "query=\"对这些专利做一个分类？\"\n",
    "promptTemplate = PromptTemplate(input_variables=[\"query\"],template=\"以下请用中文回答，尽量采用表格或列表回答\\n{query}\")\n",
    "find_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain = load_qa_chain(stream_openai, chain_type=\"refine\")\n",
    "r = chain.run(input_documents=find_docs,question=promptTemplate.format(query=query))\n",
    "print(r.encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "\n",
    "with open(\"../../state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
    "from langchain.chains import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0,model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "chain_new = APIChain.from_llm_and_api_docs(chat, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n",
    "chain_new.run('How is the weather in Xiamen，chain today?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# streaming llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], model_name=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "resp = chat(\"写一首狗和猫的诗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMRequestsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMRequestsChain,LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Between >>> and <<< are the raw search result text.\n",
    "Extract the answer to the question '{query}' or say \"not found\" if the information is not contained.\n",
    "Use the format\n",
    "Extracted:<answer or \"not found\">\n",
    ">>> {requests_result} <<<\n",
    "Extracted:\"\"\"\n",
    "\n",
    "promptObj = PromptTemplate(\n",
    "    input_variables=[\"query\", \"requests_result\"],\n",
    "    template=template,\n",
    ")\n",
    "chain = LLMRequestsChain(llm_chain = LLMChain(llm=stream_openai, prompt=promptObj))\n",
    "#question = \"What are the Three (3) biggest countries, and their respective sizes?\"\n",
    "question = \"langchain是什么，给一个python代码\"\n",
    "#url = \"https://www.baidu.com/s?wd=\"\n",
    "url = \"https://www.google.com/search?q=\"\n",
    "inputs = {\n",
    "    \"query\": question,\n",
    "    \"url\":  url + question.replace(\" \", \"+\")\n",
    "}\n",
    "chain(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchainHub （不建议，因为默认配置采用davici-0003，贵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain\n",
    "chain = load_chain(\"lc://chains/llm-math/chain.json\")\n",
    "chain.run(\"5的立方根\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output_parser\n",
    " 'CommaSeparatedListOutputParser',\n",
    " 'GuardrailsOutputParser',\n",
    " 'ListOutputParser',\n",
    " 'OutputFixingParser',\n",
    " 'PydanticOutputParser',\n",
    " 'RegexDictParser',\n",
    " 'RegexParser',\n",
    " 'ResponseSchema',\n",
    " 'RetryOutputParser',\n",
    " 'RetryWithErrorOutputParser',\n",
    " 'StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"列出所有彩虹的颜色\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[], output_parser=output_parser)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=openai)\n",
    "\n",
    "print(llm_chain.predict())\n",
    "print(llm_chain.predict_and_parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=get_llm(name=\"openai\"), chain_type=\"refine\", retriever=docsearch.as_retriever())\n",
    "query = \"以下请用中文回答\\n\\n这些专利中哪些专利是最接近的?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "import pandas as pd\n",
    "#600029_order.csv\n",
    "#600029_tick.csv\n",
    "#600029_trade.csv\n",
    "df = pd.read_csv('600029_order.csv')\n",
    "agent = create_pandas_dataframe_agent(openai, df, verbose=True)\n",
    "agent.run(\"增加一个列标记volume超过平均值\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gradio\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "def upload_image(image_file):\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.mkdir(\"uploads\")\n",
    "    image_file.save(\"uploads/\" + image_file.name)\n",
    "    return \"上传成功！\"\n",
    "\n",
    "#关闭所有已经创建的网站，释放端口\n",
    "gr.close_all()\n",
    "#定义接口\n",
    "iface = gr.Interface(upload_image, \n",
    "                     inputs=\"file\", \n",
    "                     outputs=\"image\",\n",
    "                     title=\"上传图片网站\", \n",
    "                     description=\"上传图片并保存到本地uploads文件夹中。\")\n",
    "#启动\n",
    "iface.launch(share=True,server_port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Audio??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gradio/templates.py:32: UserWarning: You have unused kwarg parameters in TextArea, please remove them: {'default': 'hello\\nworld'}\n",
      "  super().__init__(\n",
      "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:760: UserWarning: Expected 3 arguments for function <function new_message at 0x7f73dcf62290>, received 1.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:764: UserWarning: Expected at least 3 arguments for function <function new_message at 0x7f73dcf62290>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:8080\n",
      "Running on public URL: https://3762992fdac25f879e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3762992fdac25f879e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def new_message(name,is_stream,tempeture):\n",
    "    stream = \"stream\" if is_stream else \"no stream\"\n",
    "    message = f\"{name} with {stream} and {tempeture}\"\n",
    "    return message\n",
    "def change_image(image_array):\n",
    "    image = Image.fromarray(image_array)\n",
    "    bw_image = image.convert('L')\n",
    "    bw_array = np.array(bw_image)\n",
    "    return bw_array\n",
    "    #return np.fliplr(image_array)\n",
    "#关闭所有已经创建的网站，释放端口\n",
    "#gr.close_all()\n",
    "#定义接口\n",
    "with gr.Blocks() as block:\n",
    "    with gr.Tab(\"计算\"):\n",
    "        name = gr.Textbox(label=\"姓名\",placeholder=\"输入姓名\")\n",
    "        with gr.Row():\n",
    "            isStream = gr.Checkbox(label=\"Is Stream?\")\n",
    "            tempeture = gr.Slider(label=\"温度\",minimum=0,maximum=1,step=0.1)\n",
    "        msg = gr.Textbox(label=\"输出的消息\")\n",
    "        with gr.Row() as row:\n",
    "            gr.Button(\"计算\").click(fn=new_message,inputs=[name,isStream,tempeture],outputs=[msg])\n",
    "            gr.Button(\"关闭\").click(fn=lambda : block.close())\n",
    "    with gr.Tab(\"问答\"):\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"# MarkDown \\n ## Step1 提问 \\n *划重点* \\n ## Step2 回答 \\n `hello world`\")\n",
    "            question=gr.TextArea(label=\"文字编辑\",default=\"hello\\nworld\")\n",
    "            m1=gr.Markdown()\n",
    "            gr.Button(\"生成\").click(fn=new_message,inputs=[question],outputs=[m1])\n",
    "    with gr.Tab(\"媒体\"):\n",
    "        with gr.Accordion(label=\"地址\"):\n",
    "            gr.HTML(\"<p style='color:red'>包括图片、声音、视频</p>\")\n",
    "            with gr.Column():\n",
    "                with gr.Row():\n",
    "                    image=gr.Image()\n",
    "                    bw_image = gr.Image()\n",
    "                gr.Button(\"change\").click(fn=change_image,inputs=[image],outputs=[bw_image]) \n",
    "            with gr.Column():\n",
    "                vedio=gr.Video()\n",
    "                gr.Button(\"change\").click(fn=lambda:print(\"ok\"),inputs=[],outputs=[]) \n",
    "            audio=gr.Audio()\n",
    "                \n",
    "            \n",
    "\n",
    "#启动\n",
    "block.launch(share=True,server_port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY must be set\")\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from gradio_tools import (StableDiffusionTool, ImageCaptioningTool, StableDiffusionPromptGeneratorTool,\n",
    "                          TextToVideoTool)\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "tools = [StableDiffusionTool().langchain, ImageCaptioningTool().langchain,\n",
    "         StableDiffusionPromptGeneratorTool().langchain, TextToVideoTool().langchain]\n",
    "\n",
    "\n",
    "agent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\n",
    "output = agent.run(input=(\"Please create a photo of a dog riding a skateboard \"\n",
    "                          \"but improve my prompt prior to using an image generator.\"\n",
    "                          \"Please caption the generated image and create a video for it using the improved prompt.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
